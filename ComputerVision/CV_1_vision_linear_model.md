# CV_1_vision_linear_model

## Linear Classifiers

CIFAR-10 데이터 세트는 각각이 작은 이미지인 오만개의 훈련 이미지와 만 개의 테스트 이미지를 포함하는 데이터 세트이다.

각 이미지는 32*32 픽셀이면 각 픽셀은 RGB 색상 채널로 이루어져 있다. 32x32x3



선형 분류기는 매개변수로 이미지를 접근한다.

입력(input) 이미지를 픽셀 x와 학습 가능한 가중치 W와 함께 함수 F를 구성한다.



이미지는 3072(32x32x3)개의 픽셀로 구성된다. 해당 픽셀을 긴 벡터로 stretch 펼친다. (stretch pixels into column)

![image](https://github.com/hhzzzk/studyLog/assets/67236054/a6976ffc-e7da-42f6-be8d-ffe650cec9a8)

f(x,W)는 선형 분류기의 출력이다.

최종 출력이 클래스10개의 분류로 되어야 하므로 행렬의 크기가 위와 같아진다.

이미지는 3072개의 픽셀로 구성된다.

최종 출력인 (10,)의 벡터 출력의 각 원소는 각 클래스에 대한 점수를 나타낸다.



![image](https://github.com/hhzzzk/studyLog/assets/67236054/d5a3e801-48bb-4cdd-a9f8-fb5c7599cf2f)



편향을 데이터 벡터에 추가하고 편향은 가중치 행렬의 마지막 열로 흡수된다.

편향 b를 따로 유지하지 않고 가중치 행렬 W'인 새로운 가중치 행렬로 W'x 식으로 표현 가능하다.

그럴 경우 W'는 편향을 나타내는 값이 추가되어 크기가 (10,3073)이 된다.



선형 분류기에서의 예측값은 선형 변환이므로 항상 선형성을 유지한다.



## Interpreting a Linear Classifier

선형 분류기에서는 이미지의 픽셀을 열로 스트레치, 펼쳤다. (이미지 > 벡터 열)

반대로 가중치 행렬 W의 행을 이미지로 합칠 수 있다. (벡터 행 > 이미지)

즉 가중치 행렬의 각 행을 하나의 이미지로도 해석할 수 있다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/c10585f1-d135-47ce-9847-5f9b215a82a7)

가중치 행렬 W는 각 행이 하나의 클래스에 해당하며 각 열을 특정 픽셀에 대한 가중치를 나타낸다.

각 행을 이미지로 펼칠 경우 해당 클래스가 어떤 패턴이나 모양에 민감한지(어떤 특징에 민감한지) 시각적으로 확인 가능하다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/c305233f-7712-4f1c-8199-8a6669e00ce9)



가중치 행렬의 각 행을 이미지로 생성하면 각 클래스당 하나의 템플릿!!!이 나온다. 위에서 나온 어떤 특징에 민감한지를 나타낸 이미지

선형 분류기는 각 클래스마다 하나의 행렬 템플릿을 가진다.

그러나 단일 템플릿으로는 데이터를 제대로 나타낼 수 없다. 

선형 분류기는 선형 결정 경계를 가진다. 선형적 패턴은 잘 처리한다. 그러나 실제 데이터는 선형성과 비선형성을 모두 포함한다. 그러므로 비선형 분류기나 신경망 같은 모델들을 이용할 수 있다.

> 선형 분류기는 템플릿 매칭과 유사하다

## Recall: Perceptron couldn’t learn XOR

퍼셉트론은 간단한 선형 분류 문제에 대해서는 잘 작동한다.

그러나 XOR 연산과 같은 비선형 문제에 대해서는 제대로 학습하지 못한다.

XOR 연산은 두 입력 비트가 다를 때 1을 출력하고 같으면 0을 출력하는 논리 연산이다. 입력 공간에서 이를 만족하는 선형 결정 경계를 찾을 수 없다. 선형 분리가 불가능하다.



즉 이 문제는 비선형 문제이다. 이를 해결하기 위해 다층 퍼셉트론 등이 필요하다!

> 비선형성 해결해보자!



## So Far: Defined a linear score function    

지금까지 f=Wx+b로 선형 분류기를 정의해 각 클래스별로 스코어를 계산했다.

W는 어떻게 학습을 거쳐 갱신될가?



선형 분류기의 학습은 주로 손실함수 loss function을 사용해 수행된다. 

손실함수는 모델의 예측이 실제와 얼마나 차이나는지를 나타내는 지표다. 

학습의 목표는 W를 갱신해 손실함수를 최소화하는 것이다.



가장 대표적인 손실함수는 MSE, 크로스 엔트로피 손실 등이 있다.

## 선형 분류기의 학습 과정

1. 손실함수 정의
2. 옵티마이저 선택 : 손실을 최소화하도록 가중치를 업데이트하는 방법이다. 즉 최적화 알고리즘을 선택한다. 주로 경사하강법을 사용한다.
3. 경사하강법을 이용해 가중치를 업데이트할 방향을 알고 해당 방향으로 업데이트하며 손실함수의 값을 확인. 이를 반복한다.
4. 종료 조건에 따라 학습이 종료될 것이다.

## loss function

손실함수는 목적함수, 비용함수 등으로도 불린다.

## Cross entropy loss

크로스 엔트로피 손실은 모델이 올바른 클래스를 잘 측정했는지 나타낸다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/04d01628-3ef0-424a-9ba0-b304cc24f23b)



## softmax

선형 분류기에서 출력된 클래스별 점수를 확률로 변환하기 위해 softmax 함수를 사용한다.

이 함수는 입력값을 정규화 normalize하여 각 클래스에 대한 확률로 만든다.



스코어를 e의 지수로 올려 exp 값을 구하고 해당 값을 전체 exp한 값들의 sum으로 나눠 확률로 변환한다.



확률이므로 softmax 함수의 출력은 항상 0과 1사이에 있고 모든 클래스의 확률의 sum은 1이다

/오잉아래 내용이 왜 2챕터에....

## 크로스 엔트로피 손실

9w2

11/1

22p~?

## Loss function

손실함수 == 분류기의 성능이다.

분류기의 성능을 정량적으로 측정하는 지표다. 손실이 낮을수록 좋은 분류기이다. 

## Cross-Entropy Loss (Multinomial Logistic Regression)

분류문제에서 손실함수로 교차 엔트로피 함수, CE를 사용한다. 

다항 로지스틱 회귀는 여러 클래스로 분류하는 문제이다.

> Want to interpret raw classifier scores as probabilities

분류기의 출력은 보통 각 클래스에 대한 점수로 나타내진다. 이는 확률이 아니다. 이를 확률로 변환해 해석하도록 한다. 이를 위해 소프트맥스 함수가 사용된다. 소프트맥스 함수는 각 클래스의 점수를 확률로 변환한다. 확률이므로 모든 클래스의 확률 합은 1이 된다.

## 👕 Softmax function

![image](https://github.com/hhzzzk/studyLog/assets/67236054/ff1e3908-fbe2-45f2-a9bf-41feafe35472)

위의 소프트맥스 함수는 다중 클래스 분류 문제에서 각 클래스에 대한 확률을 계산하는데 사용된다.

s는 각 클래스에 대한 점수이고

k는 주어진 데이터 포인트 xi에 대한 예측된 클래스를 나타낸다. 

W는 모델의 가중치이다.



주어진 입력 데이터 xi에 대해 클래스 k에 속할 조건부확률이 우측 공식이다. 

분모의 시그마는 모든 클래스 j에 대한 점수의 지수함수의 합이다. 이를 통해 점수를 확률로 변환한다.



## 👕 적용

![image](https://github.com/hhzzzk/studyLog/assets/67236054/325f60c6-66b0-4528-a7d9-41ca39fc6e90)

cat은 3.2

car은 5.1

frog는 -1.7

1. 세 개의 클래스에 exp를 취한다.
2. 클래스가 3개이므로 j는 3, exp 취한 세 개의 값의 sum을 분모로 해서 확률로 변환한다 == normalize
   - normalize가 확률로 변환한다. 즉 주어진 값들을 총합으로 나눠 상대적 비율을 유지하며 확률로 변환

## 👕 교차 엔트로피 손실

엔트로피는 확률 분포의 불확싱성을 측정하는 척도로 확률 분포가 얼마나 예측하기 어려운지를 나타낸다.



우측에서 교차 엔트로피 손실을 구하는 식이 Li로 

​	주어진 입력 x에 대한 클래스 y에 속하는 조건부 확률에 -log를 붙인다.



교차 엔트로피 손실은 모델의 예측과 실제 레이블 간의 차이를 측정한다. 즉 주어진 데이터에 대한 모델의 부정확성을 의미한다. 

이 값이 높을수록 모델의 예측이 실제와 다르다는 의미이다. 

## 👕 MLE

![image](https://github.com/hhzzzk/studyLog/assets/67236054/570fbfed-9e37-4dd2-a7c9-9a0c70ec8826)

MLE는 최대 가능도 추정, Maximum likelihood estimation은 주어진 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 방법 중 하나이다.



소프트맥스로 구한 확률상 car이 0.87로 해당 데이터는 car에 속할 가능성이 87퍼센트이다.

frog의 확률이 0이기 때문에 frog 클래스일 확률은 거의 없다.



해당하는 교차 엔트로피 손실을 계산하면

1. cat: −log⁡(0.13)≈2.040−log(0.13)≈2.040
2. car: −log⁡(0.87)≈0.139−log(0.87)≈0.139

cat이 주어진 데이터에 더 부적합하다.



> exp를 취해도 결과의 순서, order는 변하지 않는다!



!!!!!! 소프트맥스한 값이 1일때 -log는 0이 되면서 최소값이 됨

즉 소프트맥스가 1이되도록, 교차가 젤 작은 값이 되도록하는 가중치를 찾아야함