# AI_2_선형대수(1)

4w1(9/25)~

## 기계학습에서 수학의 역할

기계학습에서 사용하는 수학에는 **선형대수 / 확통 / 최적화 / 정보이론** 4가지가 있다.

- 선형대수

  - representation

    - 데이터를 벡터와 행렬로 표현하는데 사용된다. 이러한 표현 representation을 통해서 다차원 공간에서 데이터를 효과적으로 나타낼 수 있다. 기계 학습 모델의 입력으로 벡터와 행렬을 사용한다.
    - 기계 학습 모델은 파라미터(가중치)를 사용해 입력과 출력 사이의 관계를 모델링한다. 이러한 가중치를 행렬과 벡터를 사용해 표현한다. 

  - 연산

    - 선형 연산을 통해 기게학습에서 데이터를 처리하고 모델을 학습하기 위한 곱셈, 분해, 역행렬 계산 등을 수행한다. 

    - 차원의 변환과 특성 선택(가장 주된 축, 기저를 선택)하기 위해서도 사용한다.

      - 고차원 공간을 저차원으로 투영하거나 그 반대의 경우도 선형연산을 이용해 수행한다.

        > 데이터 특징 공간과 차원을 표현 및 변환



## 벡터

- 벡터를 이루는 데이터 개수가 N개면 N차원 벡터라고 한다.

- norm : 벡터의 크기/길이

  > 벡터는 크기와 방향을 가진다.

## 행렬

벡터의 확장이 행렬이다. 행렬은 여러 개의 벡터이다. 행렬을 이용해 가중치를 표현한다.

행렬은 기본 연산의 단위이다.



## 전치행렬

행렬의 행과 열을 전치한 것이다. 교환, 결합 등 법칙이 성립한다.



## 행렬을 이용한 간결한 표현

복잡한 다항식을 행렬을 이용해 간결하게 표현할 수 있다.

+벡터의 내적은 신경망 레이어에서 입력과 가중치 간의 선형 연산을 나타낸다.



## 벡터 간 거리(유사도) 측정 방법

1. 유클리드 거리
2. 맨해튼 거리
3. 코사인 거리

3가지가 있다.

벡터 간 거리를 유사도라고 한다. 벡터 간 거리이므로 최종값은 스칼라값 1개로 나온다. (벡터 아님)

### 	코사인 유사도

코사인 유사도는 내적과 유사하다. 

코사인 거리는 두 벡터 간의 __방향적 유사성__을 측정하는데 사용된다. 이는 두 벡터 사이의 각도를 기반으로 한다.

코사인 거리의 값은 -1,1사이의 범위를 가진다. 

​	두 벡터가 완전히 동일한 방향이면 코사인 거리는 0이 되고 두 벡터가 서로 수직이면 1을 가진다.



## 가중치 규제

-  L1놈과 L2놈을 사용하는 것은 주로 가중치 규제 weight regularization을 수행하고 이를 통해 *모델의 손실 함수 값을 일반화하기 위한 목적*으로 조절한다. 
-  손실 함수를 최적화해야 하는데 너무 크면 과소적합, 너무 작을 경우 과대적합이 된다. 이를 해결하기 위한 방법 중에 **(가중치) 규제**가 있다.
  - 규제를 통해 특정 weight값이 너무 커져서 일부 특징에 의존하는 현상을 방지하고 데이터의 일반적인 특징, 일반화를 잘 반영하도록 한다. 가중치 규제에는 L1 규제와 L2 규제가 있다.
  - L1규제와 L2규제는 각각 모델의 손실 함수에 L1, L2 손실 함수를 추가한 것이다.
  - **가중치 규제를 통해 가중치 값을 제한하고, 특성 선택을 통해 중요한 특성만을 고려하도록 한다.**



### 	언제 유클리드(L2)를 쓰고 언제 맨해튼(L1)을 사용할까?

1차 놈이 맨해튼을 사용한 것이고 2차 놈이 유클리드를 사용한 것이다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/24db59dc-e40c-48f6-8b09-79b60fce5397)

유클리드가 원의 형태고 맨해튼이 마름모 형태이다. 크기를 측정한 것이라 양의 값만 가진다.

[참고링크 : 최적화 - 가중치 규제](https://seongyun-dev.tistory.com/52)

### 	❄️L1 NORM❄️

![image](https://github.com/hhzzzk/studyLog/assets/67236054/d2fde8d1-09bf-4932-b7b1-84c6bd92b44c)

맨해튼 거리라고도 한다. *실제 값과 예측 값 오차들의 절대값들에 대한 합*이다.

### 		❄️L2 NORM❄️

![image](https://github.com/hhzzzk/studyLog/assets/67236054/7c87c609-fc49-44bc-9d8e-be1a165b5e55)



유클리드 거리라고도 하며 두 점 사이의 최단 거리를 측정할 때 사용한다. *실제 값과 예측값 오차들의 제곱의 합*으로 나타낸다.

### 	❄️비교❄️

![image](https://github.com/hhzzzk/studyLog/assets/67236054/89cdb92e-fcd9-4c48-befe-215c3df21021)

순서대로 유클리드, 맨해튼, 코사인이다.

맨해튼 L1은 시작점에서 끝점으로 갈 때 여러가지 방법이 존재한다.

그에 반해 유클리드 L2는 단 한가지의 방법만 존재한다.

+outlier 이상치는 일반적인 패턴에서 크게 벗어나는 데이터 포인트이다. L1 맨해튼 여러가지 길이 있으므로 이상치에 덜 민감하고 유클리드 L2는 이상치의 제곱이 거리에 영향을 끼치므로 이상치에 민감하다. 

![image](https://github.com/hhzzzk/studyLog/assets/67236054/c501766a-3db1-4651-a878-734121e8d2dc)



L1놈은 다양한 방법 중 특정 방법을 0으로 처리하는 것이 가능하다. 즉 L1놈 규제를 사용하면 일부 가중치나 특성을 0으로 만드는 것이 가능하다. 

이것이 가능한 이유는 L1놈의 등고선이 다이아몬드 모양을 가지며 꼭짓점에 위치한 가중치들은 0이 될 가능성이 크다. 꼭짓점에서 특정 가중치가 0이 되면 이를 포함한 방향으로 이동하면서도 동일한 거리를 유지할 수 있다.

> L1놈 규제를 사용하면 중요한 가중치/특성만 남이고 불필요한 가중치를 0으로 처리하는 특성 선택 feature selection을 수행할 수 있다.

또 L1놈은 오차의 절댓값을 사용해 이상치에 더 견고하다.

~

L2놈은 outlier에 더 민감해 weight가 너무 커지는 것을 방지하는 weight decay가 가능하다. 그래서 보통은 L2놈을 더 많이 사용한다.

- L1놈은 weight selection이 가능하다 (1,0,0,0)
- L2놈은 원 형태, weight값이 전체적으로 다같이 작아져 weight shrink가 발생한다. 



12p