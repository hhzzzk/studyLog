# AI_3w2_representation learning_과대소적합

44p

t실제 현실 데이터집합은 완벽한 선형이 아니다.

선형모델의 한계가 있다. 따라서 비선형 모델로의 확장 혹은 원 특징 공간의 변화가 필요하다.

❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️

## representation matter

표현 문제. 이전에 presentation model은 스스로 뭘 배울지 선택한다.

여기서는 representation. 표현의 문제가 생긴다. 표현의 문제는 무엇인가?

*데이터의 표현은 데이터를 수치적 또는 기호적인 형태로 표현하는 방법이다.* 

이런 표현법은 모델이 데이터를 이해하고 처리하는 방식에 영향을 미친다. 적절한 표현은 복잡한 문제를 간소화하고 모델의 정확성을 향상시킨다.



45p

직교 좌표계에서는 선형으로 분리할 수 없던 데이터가 원통 좌표계로 나오자 선형으로 구분이 가능해진다.

이처럼 어떻게 표현하느냐에 따라 complexity한 문제를 쉽게 만들 수 있다.



*선형 분리 불가능*은 데이터를 완벽하게 선형으로, 직선으로 분류할 수 없는 상황이다. 이런 상황에서 직선 모델은 제한된 성능을 가지게 된다. 분류 정확도가 100%가 아니라 오분류를 어느 정도 감수해야 한다. 이를 해결하기 위해 더 복잡한 모델은 사용하거나 특징 공간 변환과 같은 기술을 사용해 처리를 더 할 수도 있다.



​	**특징 공간 변환**

특징 공간 변환은 주로 행렬의 연산을 사용해 이루어진다. 선형 변환의 경우 행렬의 곱셈으로 표현된다.

선형 변환의 일반적인 형태는 y=wx+b 이다. 이런 특징 공간 변환을 이용해 비선형성을 해결할 수 있다.

❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️

## representation learning

표현 학습. 과업에 유리한 특징 공간을 !자동!으로 찾는 작업이다. 

표현의  변환  == 공간의  변화  == 특징  추출  == 추상화

---

- 데이터의 표현이나 특징을 자동으로 학습하는 프로세스이다.

표현 학습을 통해 새로운 공간으로 변환한다. 

공간의 변환에서 가장 중요한 것은 기저공간을 이해하는 것이다. 즉 필요한 정보에만 집중하는 것이다.

표현학습은 공간의 기저를 이해해 그를 바탕으로 새로운 공간으로 변환하는 것이다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/3fa4c160-4dec-4769-b03d-68b93f3af7ad)

> 개구리 사진에서 feature representation, 특징 표현데이터를 의미 있는 형태로 변환하는 과정이다. 모델이 데이터를 이해하고 학습하는데 중요하다. 데이터의 주요특성!을 강조하거나 추출해 모델의 성능을 향상시킨다. data-driven 즉 사람이 아니라 스스로 개구리의 특징을 추출한다.
>
> 이러한 과정은 정보가 추상화되는 과정과 유사하다. 필요한, 가장 주된 정보만 남기고 다른 정보를 날린다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/fd4c5086-2a40-4160-ac98-fed4729fff32)



> 딥러닝에서는 레이어 단위로 표현학습을 한다.  점진적으로 추상화하는데 주요 정보만 남기고 공간을 점차 변환시킨다. 처음에는 low-mid-high로 점진적 변화.
>
> hidden representation 은닉층은 입력과 출력 층 사이에 위치하며 모델이 데이터를 학습하고 추상화하는 중요한 부분을 담당한다. 모델이 데이터의 내재된 특징이나 패턴을 학습하고 표현하는 방식을 나타낸다.



+) 레이어 단위의 가중치 정하는 방법?

체인룰을 사용한다. 합성함수의 미분 과정과 유사. 나중에 또 배움

gradient decent 사용한다. 

이러한 과정을 prapagation 전파라고 한다. 연쇄적으로 진행된다. 

신경망 전파에는 forward, backward 2가지가 있다. 이를통해 gradient를 개선한다.

❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️

## 과대적합, 과소적합

큰 모델을 잡아서 해보고 overfitting이면 규제를 해서 크기를 줄이면서 맞춰간다.

모델이 너무 근사하면 over고 반대면 under 피팅이다.

고차원일수록 학습 모델은 자유도가 높고 표현능력이 높고 매개변수(파라미터)의 수가 많고 모델 용량이 크다.

과소적합은 모델의 용량이 작아 오차가 큰 현상이다. 과대적합은 오차가 크게 감소한다.



​	**과대적합의 문제점**

주어진 훈련 데이터집합에 거의 완벽하게 근사한다. 그러나 연습문제만 잘 푸는 것과 유사하다. 불필요한 노이즈까지 학습한 문제가 생긴다. 새로운 데이터 집합에 대해 큰 문제가 발생한다. 일반화 성능이 낮다.

모델의 용량이 크기 때문에 학습 과정에서 불필요한 노이즈에 과대 몰입해 학습한다.



​	**편향과 변동(bias, variance)**



![image](https://github.com/hhzzzk/studyLog/assets/67236054/acbcbc70-d4f0-4de8-b75d-241ee71dc048)



과소적합 : 오차가 크다(큰 편향), 여러 데이터 집합에도 비슷한 학습 모델 형태이다(낮은 분산)

과대적합 : 오차가 작다(작은 편향), 여러 데이터 집합에서 변화가 커진다(높은 변동,분산)

- 일반적으로 작은 용량의 모델이 편향 크고 변동 작다. 큰용량 모델은 편향이 작고 변동이 크다.





​	**편향과 변동은 trade-off 관계**

학습 모델의 용량이 증가할수록 편향, 오차가 줄고 변동이 커진다.

❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️❄️

## 학습 모델 선택하는 방법

1. 검증 validation 데이터 집합을 이용한다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/b8434797-633f-4af3-b98d-5f37596252dd)



- 훈련 데이터집합과 시험 데이터집합 이외에 검증 데이터집합을 분할한다.

2. 교차 검증 cross validation 데이터집합을 이용한다.

- 소수 데이터 집합인 경우로 별도의 검증 데이터 집합이 없는 상황에서 주로 쓴다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/9b1198f1-864b-4d4d-ad7b-9b247b755faf)

- 10-fold 방법은 훈련집합을 10개의 그룹으로 나누고 10개를 순서대로 검증셋으로 정하고 9개로 테스트, 이 과정을 모든 그룹에 반복한다.



## 규제 regularization

규제로 일반화 성능을 올린다. 과대적합 문제를 해결한다.  2차 모델이라면 w1x^2 + w2x+b 라면 w1을 0으로 만들어 차수를 낮출 수 있다. 즉 모델의 파라미터 일부를 사용하지 않게 할 수 있다.

+...