11_13(11w1)
과제 내일 나올듯..이번주 수요일 선배강연



39 22 / 35 1

40 15 / 36 1

41 19 /37 1

## ResNet
residual learning, 잔류 잔차 학습을 통해 성능 저하를 피하면서 은닉층 수를 늘린다.

기존 합성공 신경망 학습의 문제는 레이어를 늘릴 수록 성능이 높아진다고 기대했는데
해보니 레이어 56개인 경우와 20개를 비교했을 때
(성능은 오류의 수가 적을수록 높다)
56층이 성능이 안좋다. (training error 에러가 더 높음)

일반화 성능도 56층이 더 안좋다 (test error)

2개가 모두 안좋다는 것은 56층이 학습이 안된다는 것이다. 

학습이 된다는 전제 한에 층이 많아질수록 성능이 높아진다고 생각했는데 층이 깊어지자 학습이 안되는 문제 발생

그래서 bypass, 자기자신이 그대로 가는 경로를 추가한다.
경로가 2개

자기자신이 그대로 가는 경로면 최소한 이전보다 같거나 더 나아질 것이다.
필요하다면 Bypass 경로로 가서 그대로 통과시킨다.
그게 F(x)+x에서 x가 그대로 가는 바이패스 경로
F(x)는 자기자신에서 미소한 변화, 전체가 아닌 자기자신에서 바뀌는 것만 학습한다. 그래서 잔차.
자기자신의 미소한 변화량

58p
지름길 연결, skip connection
x+F(x)미분하면 1+어쩌구 나올것

bypass 덕분에 미분하면 1나옴. 경사소멸 문제 완화 가능 

블럭단위, 풀링은 글로벌 average 풀링 > 피처맵 단위의 추상화 가능

지름길 연결하면 output gradient 보존, gradient highway라고도 함.
신경망 최적화 이점

59p
conv, pool, fc(완전연결)
풀링은 피처맵 다위, 가로 세로 크기를 줄인다.
작은 신경망
코드도 보기..

# 4. 심층 학습 특징
종단간 최적화된 학습, 계층화된 특징, 은닉층 역할 구분

62p
픽셀 단위 인식, 
sementic seg(객체 한개)와 instance seg (여러객체?)

class와 position, 둘 다 잘해야함
교집합

목적함수 정의..해서..loss구하고 등
64p
생성형 모델, 분류형 모델
생성 모델의 목표
입력에 집중,

65p 직접 해보기..

67p
GAN, 생성적적대 신경망
G가 위조 지폐 생성하고
D가 경찰, 가짜 진짜 구별함
G와 D의 대립

요새는 디퓨전 모델 많이 쓴다
