# AI_5_Training_Art_for_Deep_Learning

진도_13w2_11.29수(65p~끝까지) + 6챕터 약간



55p 중요. 직접해보기

# 4 규제 : Dropout

드롭아웃은 신경망에서 사용되는 정규화(규제) 기법 중 하나로 오버피팅을 줄이는데 주로 사용된다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/7f6fac5d-5a25-4f17-85cd-a2b071db9014)



신경망의 일부 뉴런을 랜덤하게 선택해 제거한다.

1. 랜덤 뉴런 제거 : 각 뉴런을 제거할 확률 p는 보통 0.5이다. 확률에 따라 선택된 뉴런을 해당 단계에서 제거된다. 학습에 사용되지 않는다.
2. 부분 네트워크 학습 : 신경망의 랜덤 제거로 인해 새로운 구성의 *부분 신경망*이 각각 학습되는 효과가 생긴다.
3. 앙상블 효과 : 각 학습 단계에서 다양한 뉴런 부분집합에 대해 모델이 학습된다. 이는 앙상블 학습과 유사한 효과 > 모델의 일반화 능력 향상, 과적합 줄여준다.



드롭아웃은 보통 완전 연결층(fully connected layer)에 적용되지만 CNN이나 RNN 등의 신경망에도 적용될 수 있다. 드롭아웃을 통해 모델의 일반화 능력을 향상시키고 훈련 데이터 의존성을 줄일 수 있다.



## Dropout : mean activation, activation 비교

![image](https://github.com/hhzzzk/studyLog/assets/67236054/fd970ae1-a2f5-485b-a9fe-afffb89ed25e)

평균 활성화

- 드롭아웃 적용 전에는 모든 뉴런이 학습에 참여하므로 전반적인 활성화 수준이 높다.
- 드롭아웃 적용 후에는 일부 뉴런이 제거되므로 평균 활성화가 낮아진다.
  - 이는 모델이 각 학습 단계에서 일부 뉴런만 사용해 더 일반화 효과를 이끌어낸다.

활성화 분포

- 적용 전에는 전체 범위에 걸쳐 분포한다.
- 적용 후에는 일부 뉴런이 제거되어 더 sparse하고 부분적이다. 



##Dropout : 인공신경망의 완전연결층에 드롭아웃을 적용한 알고리즘

드롭아웃은 주로 training 훈련 단계에서 사용된다. 

- 훈련 단계에서는 일부 뉴런을 제거해 학습을 돕는다.

테스트 단계에서는 비활성화된다.

- 테스트 시에는 전체 네트워크를 사용해 예측을 수행해야 한다.



왜?

- 드롭아웃을 훈련과 테스트에서 모두 사용할 경우 모델이 더 적은 노드로 작동하게 되어 테스트의 성능은 향상될 것이다.
- 그러나 테스트와 훈련 환경에서의 모델 동작이 달라진다.
  - 드롭아웃을 적용할 경우 테스트 단계에서 사용되는 가중치의 기대값과 훈련 단계에서 사용된 가중치의 기대값이 다를 수 있다.
    - 이를 보정하기 위해 주로 테스트 단계에서 드롭아웃 비율을 고려해 훈련된 가중치를 조정한다.
    - 훈련시에는 드롭아웃 확률을 고려해 뉴런을 제거하고 테스트할 때는 해당 드롭아웃 비율을 곱해 가중치를 보정한다.
      - 이를 통해 훈련과 테스트 간에 일관성을 유지하며 모델 성능 평가가 가능하다.

---

![image](https://github.com/hhzzzk/studyLog/assets/67236054/9605909e-294c-4731-a3eb-278e02b876c7)

- 보통 드롭아웃은 입력층과 은닉층에 다른 드롭아웃 비율을 적용한다. 해당 비율은 각 노드의 드롭아웃, 제거될 확률이다.
  - 여기서는 Pinput = 0.2, Phidden = 0.5

![image](https://github.com/hhzzzk/studyLog/assets/67236054/d02e9ea7-fd7f-4e27-a72a-f3037df294db)

- 테스트 때는 모든 노드가 존재







# 5. 하이퍼 매개변수 설정

기계학습모델의 두 가지 매개변수

1. 내부 매개변수(가중치)
   - 모델의 학습과정에서 최적화되는 변수, 가중치라고도 한다.
   - 훈련 데이터 집합에 기반해 학습 알고리즘으로 갱신됨
2. 하이퍼 매개변수
   - 모델의 외부에서 주어지며 사람이 설정하는 값
   - 모델의 구조나 학습 동작 조절에 사용.
   - 대표적으로 은닉층의 개수, CNN의 필터 크기와 보폭, 학습률 등
   - 하이퍼 파라미터는 모델의 성능과 학습 특성에 큰 영향을 미친다.
3. 비교
   - 모델 내부에서 학습됨 VS 외부에서 사람이 결정
   - 경사하강법으로 최적화 VS 실험과 경험적으로 조정

![image](https://github.com/hhzzzk/studyLog/assets/67236054/c9f97b8d-7630-4c18-baf4-361c7f7273c9)

## 5 : 하이퍼 매개변수 선택

주로 표준 참고 연구가 제시하는 기본값을 참고한다. 여러 후보값 중 주어진 데이터에 가장 최적인 값을 선택한다. 이러한 과정을 하이퍼 매개변수 최적화라 한다.

5줄 valid set으로 성능을측정한다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/f35f793e-ff81-4c8d-a6b1-9f4c10ce6efd)

하이퍼 매개변수 최적화 방법 4가지

3줄(하이퍼 매개변수 조합 생성)을 구현하는 방법에 따라 수동 탐색, 격자 탐색, 임의 탐색이 있다.

- 최근에는 하이퍼 파라미터 최적화도 자동화시키는 방법들이 연구 중이다.



## 5 : 그리드 서치, 랜덤 서치 비교

![image](https://github.com/hhzzzk/studyLog/assets/67236054/1a3c7418-a707-4f6b-a6e4-e512ca14fdae)

격자 탐색(그리드 서치)

- 미리 정의된 하이퍼 파라미터 값들의 조합을 모두 시도한다. 모든 조합을 테스트하기 때문에 전체 탐색 공간을 탐색한다.
- 조합이 많은 경우 계산 비용이 높을 수 있다. 고차원의 조합 탐색에서 비효율적



임의 탐색(랜덤 서치)

- 난수를 사용해 조합을 생성한다. 특정 범위 내에서 랜덤으로 조합을 선택해 계산 비용이 낮다.
- 기계학습의 경우 하이퍼 파라미터가 m개이고 q개의 구간을 가진다면 q^m개의 점을 탐색해야 한다. 매우 큰 탐색공간이다. 랜덤 서치는 이런 공간에서도 효과적이다.



로그 공간

- 로그 공간을 사용해 간격을 조절할 수 있다. 로그 간격으로 탐색하는 것은 학습률 등의 하이퍼 파라미터에서 유리하다.
- 0.0001부터 1까지의 균일한 간격(uniform)으로 탐색할 경우 작은 값들의 영향력은 매우 작아 무시될 수 있다. 로그 공간에서의 간격(log-uniform)을 사용하면 작은 값이므로 2배씩 증가시켜 더 균형이 맞다

---

![image](https://github.com/hhzzzk/studyLog/assets/67236054/aef2f31f-488c-48b6-9ebd-4e00034af339)

uniform VS log-uniform

- 유니폼 분포는 특정 범위 내에서 모든 값이 균등하게 나타나는 분포이다.
- 로그 유니폼 분포는 로그 스케일에서 균일하게 나타나는 값들의 분포이다.
  - 작은 값에서는 큰 간격을, 큰 값에서는 작은 간격을 나타내 균형 있는 분포가 된다. 가변성을 추가!



> 그리드보다 랜덤 서치가 효율적
>
> 유니폼보다 로그유니폼이 더 효율적!



## 5 : 하이퍼 파라미터 탐색 전략

coarse-fine 탐색 전략은 초기에는 크게크게 넓은 범위를 빠르게 탐색하고, 그 후에는 fine하게 세밀하게 탐색하는 전략이다.

그림에서 검정 점으로 coarse하게 탐색하고 빨간 범위를 fine하게 탐색

![image](https://github.com/hhzzzk/studyLog/assets/67236054/1d1c055f-0f30-4652-ab31-f11f5be5af69)



하이퍼 파라미터 선택의 5단계

1. check initial loss
   - 초기손실을 확인한다. weight decay 가중치 감소를 비활성화하고 초기화 단계의 loss을 확인한다. 즉 가중치 감쇠가 없을 때의 모델 초기 손실을 확인한다.
   - 초기 손실은 모델이 아직 어떤 데이터에도 적응되지 않은 상태의 손실이다.
2. overfit a small sample
   - 모델을 작은 훈련 데이터 샘플에 대해 오버피팅시킨다. 100프로의 훈련 정확도를 달성시킨다.
   - loss가 줄어들지 않는다면 LR(학습률)이 너무 낮거나 초기화가 잘못되었을 수 있다. 반대로 loss가 무한대로 증가하거나 NaN으로 발산하면 LR이 너무 높거나 초기화가 잘못된 것이다.
3. find LR that makes loss go down
   - 이전 단계에서 사용한 아키텍처를 사용하고 모든 훈련 데이터를 사용한다. 작은 weight decay를 적용, 100번의 반복 내에서 loss가 감소하는 LR을 찾는다.
4. coarse grid, train for 1-5 epochs
   - 3단계에서 작동한 몇 개의 LR과 weight decay를 선택해 각각에 대해 모델을 훈련시킨다.
5. refine grid, train longer
   - 4단계에서 가장 좋은 모델을 선택해 오래 훈련시킨다.
6. look at loss and accuracy curves
   - loss가 어떻게 떨어지는지. 빨리 떨어지는지 느린지 등 경향성을 확인해 조정방법을 선택한다. (coarse/fine)





## 5 : 인공신경망의 학습 전략

신경망의 훈련은 iterative, 반복적인 과정

1. setup
   - 데이터 전처리, 가중치 초기화, 규제/정규화
2. monitor training dynamics
   - 훈련 도중 데이터 분포의 변화 및 정규화를 모니터링한다.
   - 최적화 알고리즘으로 모델 매개변수를 업데이트
   - 하이퍼 파라미터 튜닝
3. evaluate and improve
   - 모델 앙상블, 훈련 데이터를 증강하거나 전이학습 등으로 성능을 향상, 디버깅으로 문제 해결 및 모델 안정성 향상

이러한 과정으로 신경망 모델을 반복적으로 훈련하고 최적화해 성능을 높이고 일반화 능력을 향상시킨다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/e7bb5fe0-a820-4c0d-867b-d2da226f884e)





# 6. 2차 미분 정보 기반의 매개변수 최적화 기법

## 6 : 뉴턴 방법

지금까지 주로 배웠던 경사 하강법은 1차 미분을 사용했다.

1차 미분의 한계는 방향만 알 수 있고 얼마나 이동해야 하는지 크기 정보는 모른다.

이를 개선하는 방법에 2차 미분 정보를 활용하는 방법이 있다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/cd04113a-87b9-4684-a5a8-7088d0614411)

- 뉴턴 방법은 2차 미분 정보를 활용해 경사하강법보다 더 빠른 경로를 알아낸다. == 빠르게 최적화 가능
  - 그러나 헤시안 행렬을 계산하는 과정이 추가된다. 계산 비용이 추가된다.



## 6 : 1차 미분 최적화와 2차 미분 최적화 비교

![image](https://github.com/hhzzzk/studyLog/assets/67236054/75d91157-a445-4426-87eb-3051be4ad3cf)

2차 미분으로 오목, 볼록함수 판별이 가능하다.

점에 접하는 부분을 통해 포물선으로 근사 가능하다. 



## 6 : 뉴턴 방법 정의

뉴턴 방법은 테일러 급수를 이용해 함수의 근사치를 찾는 최적화 알고리즘이다. 여기서는 2차 미분 값이 필요하므로 2차까지의 도함수 정보만을 사용한다.

1. 테일러 급수 적용

   - 테일러 급수는 특정점의 미분계수를 계수로 가지는 다항식의 극한

   ![image](https://github.com/hhzzzk/studyLog/assets/67236054/6e84a7e4-5678-4b34-814b-2556df9dda29)

   - (𝑤  + 𝛿)에서 𝛿는 미소한 량, 매우 작은 양을 의미한다. 

2. 매개변수를 복수로 확장하면 헤시언 행렬이 된다.

   - 헤시언 행렬은 2차 도함수의 정보를 가지고 있다.

   ![image](https://github.com/hhzzzk/studyLog/assets/67236054/44f1156e-0e32-4d38-803c-9404e4f6f7a8)

3. 𝛿으로 미분한다.

   ![image](https://github.com/hhzzzk/studyLog/assets/67236054/ed77c2ff-13d4-4519-ac2a-4f5833f75464)

   ​

   (𝑤  + 𝛿)가 최소점이라면 해당 위치의 1차 도함수는 0이 된다.

   즉 3번에서 미분한 값이 0이 되는 지점을 찾아낸다.

   ![image](https://github.com/hhzzzk/studyLog/assets/67236054/7cf383eb-b2af-454a-bffb-131f76695a91)

정리하면 위의 헤시안 역행렬과 그래디언트의 곱으로 나타낼 수 있다.



77p 시험 나올 수도. 직접해보기



## 6 : 뉴턴 방법의 한계

헤시안 행렬의 역행렬을 구하려면 time complexity가 O(m^3)인 매우 큰 연산량이 필요하다.

그래서 유사 뉴턴 방법을 사용한다.



## 6 : 유사 뉴턴 방법

직접적으로 헤시안을 찾는게 아니라 반복을 통해 근사를 찾는다.

- L-BFGS를 많이 사용. 