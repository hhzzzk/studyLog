# AI_2_최적화

## 수학의 최적화

제약조건 constraints이 있을 수도 있는 상황에서 함수의 최대치와 최소치를 찾는 것



## 기계학습에서의 최적화

기계학습 훈련 과정은 목적함수 최적화를 위한 매개변수(가중치) 공간 탐색하는 과정이다.

학습 모델의 매개변수 공간은 기계학습의 매개변수, 가중치와 편향으로 구성된 공간이다.

적절한 모델을 선택하고 모델의 성능을 평가할 때 적절한 목적함수를 선택한다.

매개변수 공간을 탐색해 *목적함수가 최저가 되는 최적점*을 찾는다. 

이 과정에서 최적화 기법을 사용하는데 최적화를 통해 모델을 훈련시키는 동안 매개변수를 업데이트하며 개선해나간다.



### ♣️모델에서의 적용♣️

![image](https://github.com/hhzzzk/studyLog/assets/67236054/5d6402e0-5d3f-421e-91e5-737168e7e318)



- 선형회귀는 목적함수로 MSE를 사용한다. 이 경우 유일해를 가지고 도함수가 0이 되는 지점이 최적점이다. 역행렬로 쉽게 구할 수 있다. (MSE는 convex 볼록한 함수이므로 최소값이 한 개만 존재한다.)
- 그 밖의 퍼셉트론이나 딥러닝 등에서는 목적함수로 경사하강법을 사용해 최적의 가중치(w, 스칼라/벡터)를 찾는다. 경사하강법은 함수를 미분한 기울기 정보를 사용해 손실함수(==목적함수)를 최소가 되는 방향으로 이동시킨다.



## 기계학습 최적화 전략

1. **낱낱탐백 exhausitive search 알고리즘**
   - 그리디하게 모든 케이스를 돈다
2. **무작위탐색 random search 알고리즘**
   - 무작위로 해를 생성해 돈다.
3. **경사하강법 gradient descent 알고리즘**
   - 손실함수가 최소화되는 방향으로 업데이트한다.
     - 방향은 미분의 음의 방향



## 경사하강법과 미분

미분한 함수, 도함수를 통해  목적함수가 최저점이 되는 방향으로 업데이트한다.

dθ로 -f'x를 사용한다. 그냥 도함수는 함수의 기울기, 값이 커지는 방향을 나타낸다. 음의 방향으로 가면 목적함수가 작아지는 최적점을 향하게 된다.



### ♣️경사도 gradient♣️

w는 여러개, 즉 벡터다. 

목적함수는 예측값과 실제값의 차이 distance이므로 결과가 스칼라이다.

w벡터와 목적함수는 함수관계이다.

즉 *변수가 여러 개인 함수이므로 미분하려면 편미분을 이용해야 한다.*



### ♣️야코비언 행렬♣️

`야코비언 행렬은 다변수 함수의 도함수들을 모아놓은 행렬이다.`

입력과 출력이 모두 다차원이다. 

이 때 가능한 미분쌍을 모두 찾아 편미분한다. 



## 미분의 연쇄법칙 chain rule

합성함수의 미분은 제일 밖의 함수를 미분하고 변수인 함수를 미분하고 미분하고를 반복하는 형태

### ♣️기계학습의 활용♣️

인공 신경망은 합성함수이다.

기계학습에서 입력 x와 w 가중치 값을 연산한 결과인 예측값을 계산하고 실제값과 차이를 비교(손실함수)해 다시 w를 갱신한다.

즉 뒤에서부터 앞으로 갱신되는 합성함수의 형태와 유사하다.  이러한 방향을 backward라고 하며

가중치의 경사도를 계산할 때 연쇄법칙이 적용되는 것을 오류 역전파 error backpropagation이라고 한다.



## 경사하강법

- 경사도를 활용해 학습 모델의 최적 매개변수를 탐색하는 반복 최적화

미분의 음의 방향으로 탐색. 이동하는 정도는 lr, leraning rate로 조절

### ♣️BGD♣️

배치 경사 하강 알고리즘

전체 데이터셋을 보고 모든 기울기의 평균을 구한다. total sum의 평균. full-batch.

방향성이 좋다는 장점이 있으나 한 번 가중치 갱시하기에 비용이 너무 많이 든다.

### ♣️SGD♣️

스토케스틱 경사 하강 알고리즘

확률을 이용한다. 데이터셋에서 임의의 값, 랜덤값을 골라 기울기를 구해서 가중치를 개싱한다.

이를 반복해 대표성을 띄도록 한다. 빠르다는 장점이 있다.

+미니배치

GPU가 감당가능한 데이터수만큼의 데이터수만큼 미니배치(샘플)를 묶어서 처리한다.

