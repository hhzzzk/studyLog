# AI_2_정보 이론

정보이론 : 불확실성을 정량화한다.

기본 원리는 확률이 작을수록 더 많은 정보량을 가진다.



**자기정보** : 사건/메시지 Ei의 정보량

​			I(x) = -log₂(P(x))

**엔트로피** : 확률변수 x의 불확실성을 나타내는 엔트로피

​		전체사건 정보량의 기대값 

​		모든 사건이 동일한 확률을 가질 때 불확실성이 가장 높은 경우 엔트로피가 최대값을 가진다.

- 정보의 무질서도를 나타내는 척도이다.
- 엔트로피가 높을수록 정보가 무질서하며 예측이 어렵다.
- 불확실성의 정량화!



## 자기정보와 엔트로피 비교

주사위는 최소 3비트 필요..



## 교차 엔트로피를 손실함수에 적용

교차, 크로스 엔트로피 손실함수는 주로 분류 classification 문제에서 사용되는 손실함수 중 하나이다.

모델의 예측값과 실제 레이블 ground truth 간의 차이를 측정하는데 사용된다.

로지스틱 회귀 및 다중 클래스 분류 모델에서 활용된다.

교차라는게 모델의 예측확률 분포와 실제 레이블, 정답값 간의 차이를 비교한다는

**logits value** : 소프트맥스 함수 이전의 모델 출력값이다. 클래스별 점수/확률을 나타낸다. 소프트맥스 함수의 입력으로 사용된다. 

**softmax value** : 모델의 출력점수를 확률 분포로 변환하는데 사용한다. 확률의 합은 1

 - 지수 함수 exp(x)는 입력 x를 자연 상수 e의 지수로 변환한다. 양수값 x를 받고 결과로 양수 값을 반환한다.
- logit값을 양수로 만든다.
- 모든 로짓 값의 합으로 나눠서 확률로 변환한다.

**ground truth** : 정답 레이블. y

- CE 손실함수는 모델이 예측한 확률 분포와 실제값,정답값의 차이를 측정한다. 
- 손실은 정답값에 해당하는 클래스의 확률에 마이너스 음의 로그를 취한 값이다.

![image](https://github.com/hhzzzk/studyLog/assets/67236054/cbcdab5c-212b-455f-8488-4e7d80294831)

## 교차 엔트로피

![image](https://github.com/hhzzzk/studyLog/assets/67236054/a6f7466c-c3db-4311-a44d-0c402db463e2)



P의 확률과 Q의 자기 정보량, 두 확률 분포의 차이, 일치하는 정도를 측정하는 지표이다.

식을 전개하면 = H(P) + KL발산

H(P)값은 고정이므로 교차엔트로피를 줄여 손실함수 값을 작게! 하는 것이 목표이므로 KL발산을 줄여야 한다.

## KL발산

![image](https://github.com/hhzzzk/studyLog/assets/67236054/c49d1c73-fb33-4e22-a781-755c5b78c37e)

Kullback-Leibler Divergence

- KL다이버전스는 두 확률 분포 사이의 유사성 또는 차이를 측정할 때 사용한다.


- 두 확률 분포 간의 거리 또는 차이를 나타내며 다른 말로 "정보손실"이라고도 한다.
- P,Q가 일치하는 정도!

---

- P(x)는 확률 분포 P에서 사건 x가 발생할 확률
- Q(x)는 확률 분포 Q에서 사건 x가 발생할 확률

###🐻특징🐻

- 비대칭성 : KL발산은 P와 Q 사이의 차이를 측정하는데 사용되며 순서에 따라 값이 달라진다.
- 양수 : KL발산은 항상 0 이상이다. 두 확률 분포가 정확히 일치하면 KL발산은 0이 된다.
- 정보 손실 특징 : 두 분포가 비슷할수록 KL발산 값이 작고, 두 분포가 다를수록 KL발산 값이 크게 나타난다.

