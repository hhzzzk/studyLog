# AI_6_Recurrent_Neural_Networks

진도_13w2_11.29수(65p~끝까지) + 6챕터 약간



# 순환 신경망

시간성(time series) 데이터

- 특징이 순서를 가지므로 순차 데이터 sequential 라고도 부른다. 
  - 지금까지 다룬 데이터는 정적 데이터, 고정 길이 fixed input
- 순차 데이터는 동적이며 보통 가변 길이다. variable-length input

시간 단위의 깊은 신경망



순환 신경망과 LSTM

- 순환 신경망은 시간성 정보를 활용해 순차 데이터를 처리하는 효과적은 학습 모델이다.

- 매우 긴 순차 데이터(30단어 이상의 긴 문장)를 처리할 때는 장기 의존성 long-term dependency를 잘 다루는 LSTM을 사용한다.

  - LSTM은 선별 기억 능력을 가진다. 최근에는 Transformer 구조로 확장됐다.

    > RNN < LSTM < Transformer


진도 13w2 끝



start

진도 14w1, 12/4월

숙제 1개 더..거기에 작년 라코도 포함될듯..



6p 순차데이터, 입력길이 가변적, 요소 사이 관계성, 문맥이 중요 > 잘 추상화시키기



7p 표기법

각 샘플이 시간 순으로 누적, 입력도 복수개 출력도 복수개 가능함



8p 기계번역, x,y의 페어로 구성

한 x에는 n개의 워드로 구성, 순차데이터

언어는 수치로 변환과정 필요. 벡터로 정량화해야 함

즉 벡터라이즈 과정 필요. 벡터라이즈하는 방법 3가지

원핫코딩, bow, 하나 더 뭐드라



9p bow, 단어가방

단어의 등장빈도수 count

기계학습에는 부적절. 치명적 한계! 시간정보가 사라진다.

시간데이터는 요소간 관계정보, 순서정보가 중요한데...



원핫코딩은 해당 단어의 위치만 1로 표시

한계2가지

비효율적이고

단어간의 유사성을 측정 못함



10p 단어 임베딩

주변정보로 가운데 센터 위치 측정?

간단한 신경망으로 관계정보 취하는 예제



king이랑 man을 보면 주변 서술어가 유사함. 그래서 비슷한 것들끼리 매핑

문맥 정보 벡터라이즈.

고정 차원 안에서 유사한 것들끼리 벡터라이즈. word2vec



11p 순서가 중요. 길이가 가변적. 동적정보



12p 시간성, 가변길이, 문맥의존성



13p RNN 구조

U,V레이어

중요! recurrent edge를 가진다!!!



14p 은닉층 사이에 정보 전달

h1은 추상화된 정보를 넘긴다.

시간에 따라 전달되는 경로, recurrent edge



즉 h2는 h1과 x2를 본다.

w:선형+비선형

왜 깊은 신경망? ~



15p

8.4식, W,U,V같다? 시간에 따라 들어가는 데이터들의 공통 패턴 찾기.

어떤 반복적인, CNN의 그것처럼



그래서 시간에 따라 동일한 가중치를 적용한다!!!!!!!!!!



16p파라미터 sharing, 레이어많아져도? 파라미터 유지



17p w 가중치행렬을 반복적으로 사용. 재사용



18p 문자단위언어모델의 예

hello, 워드 자동완성 예시. 토근 char 단위로 나눔



input layer > one-hot

입력이 순차데이터

auto-regression

Wxh, x input에서 히든레이어로 가는 가중치

Whh, 히든레이어에서 히든레이어로

Why



recurrent edge와 새로운 입력을 합쳐서 출력을 낸다!!!!!!!!!



8.7 수식보기



19p

다양한 RNN 구조, 일대일, 일대다, 다대일, 다대다

다대다는 번역. many to many

다음 다드대는 실시간 매핑, 음성 자막? tts같은



출력의 개수에 따라 loss function 정의!!!!!!!!!!

신경망의 loss는 1개, 스칼라값이다 항상! 여러개를 머지했겠지



20p

21p

일대다, 다대일

인코더와 디코더로 파트가 구분됨

두 부분에서 가중치가 다르다 w1, w2

인코더 부분에서 시간데이터를 추상화시킨다.

디코더에서 원래 출력에 대응되는 출력으로 복원한다.



22p 다대다, loss는 combine된 1개



23p 8.6식

bias term도 고려

예제 8-1 직접해보기. 옛날기출!!!!!! 꼭 풀어보기



RNN동작의 예

RNN의 기억과 문맥의존성 기능

시점에서 전달되는..요소들 포함..시각화



장기문맥 의존성 - 문장이 개길면 문장 마무리가 안됨



수식 집에서 보세요 그래도 보긴해야할듯 교수님왈



39p 그림 8-1중요



41P 시간이 지나면 사라짐

문제2개

전방계산 : 장기문맥 기억 못함

오류역전파 : 그래디언트 흐름 문제

​	그래디언트 소실 문제? 솔루션으로 나온게 LSTM은 gate를 이용함



end



truncated 잘린 시간에 따라 오류 역전파를 동자기켜 그래디언트 소실문제를 완화할수있다.

이 방법이 풀배치는 비효율적인 SGD를 구하고 반복적ㅇ로 해서 대표성 가지도록 하는 거랑 유사함





## LSTM

장기 문맥 의존성 해결!!!!!

변수를 중요하면 가져가고 아니면 버린다.



3개의 게이트 존재

학습을 통해 열고닫기를 판단한다.



C는 셀, 메모리 원래는 벡터인데 여기서는 값으로 가정함

C가 중요하면 가져가고 아니면 버린다.

게이트를 이용함 



게이트를 이용해 셀에 누적한다.

후보군 candiate를 tanh에 넣음



후보군을 입력게이트에 얼마나 반영할지 과거 얼마나 반영할지를 해서 output을 낸다.



 61p 전체동작예시



## 순환 신경망 응용사례

lstm에서 더 발전하는



## 언어모델

모델링, 확률 분포



n-ㄱ램, 고전적인 방법, 유니그램 바이그램



77p

인코더와 디코더 사용



lstm을 사용해 번역 과정 학습하는 과정

1. 인코더-디코더 아키텍처 구성
   - 입력 문장을 인코더에서 고정된 크기의 특징 벡터로 압축하고 이를 디코더에서 출력 문장으로 해석한다
   - 인코더는 입력 문장 정보 압축해 문맥 정보 인코딩해 디코더로 전달
   - 디코더는 인코더에서 얻은 특징 벡터로 목적 언어의 문장을 생성.

한계

- 문장이 길어질수록 성능 저하
- 그래서 나온게 attention!!!!!!!



## 트랜스포머 : attention

관계정보를 남기겠다

중요한건 중요하게 유지시킨다. 순차데이터는 요소간 관계정보가 중요하다. 이를 위해 어텐션 층을 사용한다.

!!!!!!!!!!!!!!!!!중요

키, 쿼리, 값으로 이뤄짐!!!!!!!

키와 쿼리의 관련 정보를 스코어 점수로 계산=유사도를 측정한다.

관련이 깊을수록 스코어 점수가 올라간다.



84p 쿼리와 키로 스코어 점수가 높을수록 유사도가 높다.



normalize, 정규화 과정을 거쳐 

softmax 확률값으로 한다.





## self-attention

1. 입력 임베딩 : 문장의 단어들이 임베딩 레이어로 고차원 벡터로 변환. 이게 모델 입력
2. 키,쿼리,벨류 생성
   - 셀프 어텐션은 주어진 입력 문장의 각 단어를 위 쌍으로 변환
   - 쿼리는 현재 단어에 대한 정보 포함, 키는 다른 단어들과의 관계, 값은 실제로 모델의 출력에 영향을 미치는 정보 포함
3. 어텐션 정보 계산
   - 쿼리와 키 사이의 어텐션 정보 계산. 두 벡터 간의 유사도. >>> 셀프 어텐션 스코어
4. 어텐션 스코어 정규화
   - 점수를 정규화해 확률로 변환
     - 어텐션 점수를 스케일링, 키벡터 크기로 나눔
     - 정규화된 점수에 소프트맥스 적용해 확률 분포 얻음
5. 가중 평균 계산
   - 계산된 어텐션 점수로 벨류 벡터의 가중 평균을 계산
     - 현재 단어는 다른 단어들의 정보를 가져와 자신의 표현을 업데이트! 
     - 위에서 계산한 softmax랑 value곱해서 value값 업데이트
   - weighted sum의 결과는 셀프 어텐션 최종 출력, ==현재 단어의 표현이다.
6. 멀티 헤드
   - 트랜스포머에서는 셀프어텐션을 멀티헤드로 사용해 여러개의 서로 다른 쌍을 사용