# AI_6_Recurrent_Neural_Networks

진도_13w2_11.29수(65p~끝까지) + 6챕터 약간



# 순환 신경망

시간성(time series) 데이터

- 특징이 순서를 가지므로 순차 데이터 sequential 라고도 부른다. 
  - 지금까지 다룬 데이터는 정적 데이터, 고정 길이 fixed input
- 순차 데이터는 동적이며 보통 가변 길이다. variable-length input

시간 단위의 깊은 신경망



순환 신경망과 LSTM

- 순환 신경망은 시간성 정보를 활용해 순차 데이터를 처리하는 효과적은 학습 모델이다.

- 매우 긴 순차 데이터(30단어 이상의 긴 문장)를 처리할 때는 장기 의존성 long-term dependency를 잘 다루는 LSTM을 사용한다.

  - LSTM은 선별 기억 능력을 가진다. 최근에는 Transformer 구조로 확장됐다.

    > RNN < LSTM < Transformer


진도 13w2 끝



start

진도 14w1, 12/4월

숙제 1개 더..거기에 작년 라코도 포함될듯..



6p 순차데이터, 입력길이 가변적, 요소 사이 관계성, 문맥이 중요 > 잘 추상화시키기



7p 표기법

각 샘플이 시간 순으로 누적, 입력도 복수개 출력도 복수개 가능함



8p 기계번역, x,y의 페어로 구성

한 x에는 n개의 워드로 구성, 순차데이터

언어는 수치로 변환과정 필요. 벡터로 정량화해야 함

즉 벡터라이즈 과정 필요. 벡터라이즈하는 방법 3가지

원핫코딩, bow, 하나 더 뭐드라



9p bow, 단어가방

단어의 등장빈도수 count

기계학습에는 부적절. 치명적 한계! 시간정보가 사라진다.

시간데이터는 요소간 관계정보, 순서정보가 중요한데...



원핫코딩은 해당 단어의 위치만 1로 표시

한계2가지

비효율적이고

단어간의 유사성을 측정 못함



10p 단어 임베딩

주변정보로 가운데 센터 위치 측정?

간단한 신경망으로 관계정보 취하는 예제



king이랑 man을 보면 주변 서술어가 유사함. 그래서 비슷한 것들끼리 매핑

문맥 정보 벡터라이즈.

고정 차원 안에서 유사한 것들끼리 벡터라이즈. word2vec



11p 순서가 중요. 길이가 가변적. 동적정보



12p 시간성, 가변길이, 문맥의존성



13p RNN 구조

U,V레이어

중요! recurrent edge를 가진다!!!



14p 은닉층 사이에 정보 전달

h1은 추상화된 정보를 넘긴다.

시간에 따라 전달되는 경로, recurrent edge



즉 h2는 h1과 x2를 본다.

w:선형+비선형

왜 깊은 신경망? ~



15p

8.4식, W,U,V같다? 시간에 따라 들어가는 데이터들의 공통 패턴 찾기.

어떤 반복적인, CNN의 그것처럼



그래서 시간에 따라 동일한 가중치를 적용한다!!!!!!!!!!



16p파라미터 sharing, 레이어많아져도? 파라미터 유지



17p w 가중치행렬을 반복적으로 사용. 재사용



18p 문자단위언어모델의 예

hello, 워드 자동완성 예시. 토근 char 단위로 나눔



input layer > one-hot

입력이 순차데이터

auto-regression

Wxh, x input에서 히든레이어로 가는 가중치

Whh, 히든레이어에서 히든레이어로

Why



recurrent edge와 새로운 입력을 합쳐서 출력을 낸다!!!!!!!!!



8.7 수식보기



19p

다양한 RNN 구조, 일대일, 일대다, 다대일, 다대다

다대다는 번역. many to many

다음 다드대는 실시간 매핑, 음성 자막? tts같은



출력의 개수에 따라 loss function 정의!!!!!!!!!!

신경망의 loss는 1개, 스칼라값이다 항상! 여러개를 머지했겠지



20p

21p

일대다, 다대일

인코더와 디코더로 파트가 구분됨

두 부분에서 가중치가 다르다 w1, w2

인코더 부분에서 시간데이터를 추상화시킨다.

디코더에서 원래 출력에 대응되는 출력으로 복원한다.



22p 다대다, loss는 combine된 1개



23p 8.6식

bias term도 고려

예제 8-1 직접해보기. 옛날기출!!!!!! 꼭 풀어보기



RNN동작의 예

RNN의 기억과 문맥의존성 기능

시점에서 전달되는..요소들 포함..시각화



장기문맥 의존성 - 문장이 개길면 문장 마무리가 안됨



수식 집에서 보세요 그래도 보긴해야할듯 교수님왈



39p 그림 8-1중요



41P 시간이 지나면 사라짐

문제2개

전방계산 : 장기문맥 기억 못함

오류역전파 : 그래디언트 흐름 문제

​	그래디언트 소실 문제? 솔루션으로 나온게 LSTM은 gate를 이용함



end